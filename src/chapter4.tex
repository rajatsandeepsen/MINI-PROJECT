\chapter{Results and Discussions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%  NEW SECTION   %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{equation}{0}

The aim of the system is to provide users with a faster, more reliable and efficient solution for navigating and performing tasks within a banking application that in turn reduces the need for manual repetitive tasks, decreases time loss, and optimizes their productivity.\\

\noindent The main platform used for the development of the system is TypeScript. LangChain, a framework in TypeScript, is used to create agent model. All the significant sections of code, excluding the front-end interface was purely implemented in NextJS, tRPC and drizzleORM. FrontEnd is server side rendered ReactJS app. Postgres from Supabase for primary database management tool. NextAuth is used for authentication\\

\noindent
Llama3 from Groq.com was used as the core language model. Initially pitched model to run on local system. But small devices like laptop are under efficient for running LLMs. Due to slow token per second, we moved to third party LLM inference provider.

\noindent A large language model (LLM) is a sophisticated artificial intelligence (AI) model that excels in natural language processing tasks. It is designed to understand and generate human-like text based on the patterns and structures it has learned from vast training data.

\noindent LLAMA3 is a significant advancement in large language models (LLMs), offering several unique features and capabilities that set it apart from its predecessors, such as surpassing both its predecessors and competitors in various benchmarks, demonstrating excellence in tasks such as Multilingual Multi-Task Learning (MMLU) and Human Evaluation (HumanEval). It has vocabulary of 128,000 words in 30+ languages and it is trained from 15 trillion token dataset be Meta (aka FaceBook). Llama3 comes in 8, 13, 70 billion parameters with instruction-tuned versions and it is open sourced.

\section{Performance Evaluation}

\noindent Any LLM with temperature at high can generate more diverse and creative text, but at low temperature, it can generate more accurate and less creative text. Here the Action model requires precision in response. So LLAMA3 is set to .1 temperature.\\

\noindent The top-p setting, also known as nucleus sampling, controls the probability threshold for selecting tokens during generation. Higher top-p values (closer to 1) prioritize more common and likely tokens, leading to safer and more predictable responses. Lower top-p values (closer to 0) allow the model to consider less common tokens, potentially generating more unique and surprising responses, but with a higher risk of incoherence. Here the top-p is set to .9.\\


\section{Comparison with Models}

\noindent Large Action Model requires 3 main components to perform well. Ability to write valid JSON format. Ability to understand the user query and with type definition of application. For that LLM requires great MMLU (Massive Multilingual Language Understanding) and fastest token per second.\\

% \begin{table}[h!]
%   \centering
%   \caption{Performance metrics comparison for two versions of CNN Models}
%   \vspace{1.5mm}
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%    & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \text{None} & \text{Overall} \\ 
%   \hline
%   \multicolumn{11}{|c|}{\text{CNN\_Model\_0}} \\ 
%   \hline
%   \text{Precision} & 0.99 & 0.99 & 0.99 & 0.99 & 0.98 & 0.99 & 0.98 & 0.99 & 1 & 0.988 \\ 
%   \hline
%   \text{Recall} & 1 & 1 & 0.98 & 0.96 & 0.99 & 0.98 & 0.99 & 0.99 & 1 & 0.987 \\ 
%   \hline
%   \text{F1-Score} & 1 & 0.99 & 0.99 & 0.98 & 0.98 & 0.99 & 0.99 & 0.99 & 1 & 0.999 \\ 
%   \hline
%   \multicolumn{11}{|c|}{\text{CNN\_Model\_1}} \\ 
%   \hline
%   \text{Precision} & 0.99 & 0.98 & 0.98 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 1 & 0.988 \\ 
%   \hline
%   \text{Recall} & 0.99 & 0.99 & 0.99 & 0.98 & 0.98 & 0.98 & 0.99 & 0.98 & 1 & 0.986 \\ 
%   \hline
%   \text{F1-Score} & 0.99 & 0.99 & 0.98 & 0.98 & 0.99 & 0.99 & 0.99 & 0.99 & 1 & 0.988 \\ 
%   \hline
%   \end{tabular}
% \end{table}


\begin{table}[h!]
  \centering
  \caption{Performance metrics comparison for Large Language Models}
  \vspace{1.5mm}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \begin{tabular}[c]{@{}l@{}}Model\\    \\ (API ID)\end{tabular} & \begin{tabular}[c]{@{}l@{}}INDEX\\    \\ (Normalized avg)\end{tabular} & \begin{tabular}[c]{@{}l@{}}CONTEXT\\    \\ WINDOW\end{tabular} & MMLU & \begin{tabular}[c]{@{}l@{}}MEDIAN\\    \\ (Tokens/s)\end{tabular} \\
    \hline
    claude-3-opus-20240229-v1:0 & 100 & 200k & 0.868 & 27.5 \\
    \hline
    gpt-4 & 90 & 8k & 0.864 & 17.5 \\
    \hline
    Llama3-70b-8192 & 88 & 8k & 0.82 & 307.3 \\
    \hline
    claude-3-sonnet-20240229-v1:0 & 85 & 200k & 0.79 & 59.4 \\
    \hline
     Mixtral-8x22B-Instruct-v0.1 & 83 & 65k & 0.77752 & 49.9 \\
    \hline
    claude-3-haiku-20240307-v1:0 & 78 & 200k & 0.752 & 85.4 \\
    \hline
    claude-instant-v1 & 65 & 100k & 0.734 & 84.2 \\
    \hline
    mixtral-8x7b-instruct & 68 & 16k & 0.706 & 117.1 \\
    \hline
    gpt-3.5-turbo & 67 & 16k & 0.7 & 58.5 \\
    \hline
    gpt-35-turbo & 67 & 16k & 0.7 & 54 \\
    \hline
    llama2-70b-4096 & 56 & 4k & 0.689 & 251.5 \\
    \hline
    llama-3-8b-instruct & 58 & 8k & 0.684 & 121.4 \\
    \hline
    Llama3-8b-8192 & 58 & 8k & 0.684 & 920.8 \\
    \hline
    Mistral\_7B\_Instruct & 40 & 4k & 0.625 & 230.6 \\
    \hline
    mistral-7b-instruct & 40 & 16k & 0.625 & 102.9 \\
    \hline
    Mistral-7B-Instruct-v0.1 & 40 & 8k & 0.625 & 77.6 \\
    \hline
    mistral-7b-instruct-v0.2 & 40 & 33k & 0.625 & 93.2 \\
    \hline
    llama-2-13b-chat & 37 & 4k & 0.536 & 115.3 \\
    \hline
    llama-2-7b-chat & 27 & 4k & 0.458 & 204.7 \\
    \hline
  \end{tabular}
\end{table}
\clearpage

\section{Discussion}

\noindent The evaluation of these models are provided by ArtificialAnalysis/LLM-Performance-Leaderboard from hugging face. The evaluation is based on the following metrics: API ID, INDEX (Normalized avg), CONTEXT WINDOW, MMLU, MEDIAN (Tokens/s). The evaluation is based on the performance of the models in terms of their ability to generate human-like text, understand and respond to user queries, and provide accurate and relevant information. The models are ranked based on their performance across these metrics, with higher scores indicating better performance.\\

\noindent But the first two models are not open source and they cost more. The third model is Llama3 (70B) which is open source and has the highest MMLU score of 0.82. It has a median token per second of 307.3. More parameters means large capacity for complex and diverse tasks including those that require multi-step reasoning and logical explanations. Also more complex and sophisticated model that can learn and capture more nuanced patterns in the data. This can lead to better performance on various tasks, such as text generation, translation, and question-answering. 

\noindent However, it is essential to note that increasing the parameter count does not always guarantee better performance. The model's architecture, training data, and fine-tuning processes also play crucial roles in determining its overall capabilities. Therefore, it is essential to consider these factors in conjunction with the parameter count to assess the model's performance accurately. From a perspective of Enterprise the model has to be open source or they can't fine tune the model with their own custom dataset. Fine tuning on their specialized environment can help to gain more context window through each instance of request.\\

\noindent This suggests that the choice of model influence on the overall performance of the application. Thus, Llama3 with 8B is perfect for the project, making it an absolute choice that doesn't need more that 16GB of RAM to execute. And it does have required MMLU with token per second. The project just require a model in-between 4B and 8B parameters.\\

\clearpage
% \end{document}